{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "fine_tune_yolo.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ZqSsw4XYX1r",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "46cfe7d6-3fc4-4571-89a0-d0b3f826f312"
      },
      "source": [
        "! pip install  xmltodict\n",
        "! pip install --upgrade albumentations"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting xmltodict\n",
            "  Downloading https://files.pythonhosted.org/packages/28/fd/30d5c1d3ac29ce229f6bdc40bbc20b28f716e8b363140c26eff19122d8a5/xmltodict-0.12.0-py2.py3-none-any.whl\n",
            "Installing collected packages: xmltodict\n",
            "Successfully installed xmltodict-0.12.0\n",
            "Collecting albumentations\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/03/58/63fb1d742dc42d9ba2800ea741de1f2bc6bb05548d8724aa84794042eaf2/albumentations-0.5.2-py3-none-any.whl (72kB)\n",
            "\u001b[K     |████████████████████████████████| 81kB 3.9MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: numpy>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from albumentations) (1.19.5)\n",
            "Requirement already satisfied, skipping upgrade: scipy in /usr/local/lib/python3.6/dist-packages (from albumentations) (1.4.1)\n",
            "Requirement already satisfied, skipping upgrade: scikit-image>=0.16.1 in /usr/local/lib/python3.6/dist-packages (from albumentations) (0.16.2)\n",
            "Collecting opencv-python-headless>=4.1.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/96/fc/4da675cc522a749ebbcf85c5a63fba844b2d44c87e6f24e3fdb147df3270/opencv_python_headless-4.5.1.48-cp36-cp36m-manylinux2014_x86_64.whl (37.6MB)\n",
            "\u001b[K     |████████████████████████████████| 37.6MB 1.8MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: PyYAML in /usr/local/lib/python3.6/dist-packages (from albumentations) (3.13)\n",
            "Collecting imgaug>=0.4.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/66/b1/af3142c4a85cba6da9f4ebb5ff4e21e2616309552caca5e8acefe9840622/imgaug-0.4.0-py2.py3-none-any.whl (948kB)\n",
            "\u001b[K     |████████████████████████████████| 952kB 28.2MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: imageio>=2.3.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image>=0.16.1->albumentations) (2.4.1)\n",
            "Requirement already satisfied, skipping upgrade: networkx>=2.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image>=0.16.1->albumentations) (2.5)\n",
            "Requirement already satisfied, skipping upgrade: pillow>=4.3.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image>=0.16.1->albumentations) (7.0.0)\n",
            "Requirement already satisfied, skipping upgrade: PyWavelets>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image>=0.16.1->albumentations) (1.1.1)\n",
            "Requirement already satisfied, skipping upgrade: matplotlib!=3.0.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image>=0.16.1->albumentations) (3.2.2)\n",
            "Requirement already satisfied, skipping upgrade: Shapely in /usr/local/lib/python3.6/dist-packages (from imgaug>=0.4.0->albumentations) (1.7.1)\n",
            "Requirement already satisfied, skipping upgrade: opencv-python in /usr/local/lib/python3.6/dist-packages (from imgaug>=0.4.0->albumentations) (4.1.2.30)\n",
            "Requirement already satisfied, skipping upgrade: six in /usr/local/lib/python3.6/dist-packages (from imgaug>=0.4.0->albumentations) (1.15.0)\n",
            "Requirement already satisfied, skipping upgrade: decorator>=4.3.0 in /usr/local/lib/python3.6/dist-packages (from networkx>=2.0->scikit-image>=0.16.1->albumentations) (4.4.2)\n",
            "Requirement already satisfied, skipping upgrade: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.16.1->albumentations) (2.8.1)\n",
            "Requirement already satisfied, skipping upgrade: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.16.1->albumentations) (2.4.7)\n",
            "Requirement already satisfied, skipping upgrade: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.16.1->albumentations) (0.10.0)\n",
            "Requirement already satisfied, skipping upgrade: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.16.1->albumentations) (1.3.1)\n",
            "Installing collected packages: opencv-python-headless, imgaug, albumentations\n",
            "  Found existing installation: imgaug 0.2.9\n",
            "    Uninstalling imgaug-0.2.9:\n",
            "      Successfully uninstalled imgaug-0.2.9\n",
            "  Found existing installation: albumentations 0.1.12\n",
            "    Uninstalling albumentations-0.1.12:\n",
            "      Successfully uninstalled albumentations-0.1.12\n",
            "Successfully installed albumentations-0.5.2 imgaug-0.4.0 opencv-python-headless-4.5.1.48\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7JWFGYlOYUIZ"
      },
      "source": [
        "import os \n",
        "import ast\n",
        "import pandas as pd\n",
        "import numpy as np \n",
        "import shutil\n",
        "from tqdm import tqdm # for progress bar\n",
        "from pprint import pprint \n",
        "import json\n",
        "import xmltodict\n",
        "import ast\n",
        "import torch\n",
        "import torch.utils.data\n",
        "from PIL import Image\n",
        "import cv2\n",
        "import albumentations as A"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T8n2RbnnX7e1",
        "outputId": "883a0e86-ef2a-456b-aea1-aa39af7a5089"
      },
      "source": [
        "!git clone https://github.com/ultralytics/yolov5  # clone repo\n",
        "%cd yolov5\n",
        "%pip install -qr requirements.txt  # install dependencies\n",
        "\n",
        "import torch\n",
        "from IPython.display import Image, clear_output  # to display images\n",
        "\n",
        "clear_output()\n",
        "print('Setup complete. Using torch %s %s' % (torch.__version__, torch.cuda.get_device_properties(0) if torch.cuda.is_available() else 'CPU'))"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Setup complete. Using torch 1.7.0+cu101 CPU\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "584OGqDGYa-r"
      },
      "source": [
        "transform = A.Compose([\n",
        "    A.Resize(512, 512)\n",
        "], bbox_params=A.BboxParams(format='yolo'))"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jf4wG7wb9twK",
        "outputId": "5436d624-50a0-4732-f194-f0e5a0595620"
      },
      "source": [
        "with open('/content/drive/MyDrive/torch/archive/test_zip/test/apple_79.xml') as fd:\r\n",
        "    bb_file = json.loads(json.dumps(xmltodict.parse(fd.read())))\r\n",
        "    pprint(bb_file)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'annotation': {'filename': 'apple_79.jpg',\n",
            "                'folder': 'test',\n",
            "                'object': {'bndbox': {'xmax': '641',\n",
            "                                      'xmin': '77',\n",
            "                                      'ymax': '716',\n",
            "                                      'ymin': '183'},\n",
            "                           'difficult': '0',\n",
            "                           'name': 'apple',\n",
            "                           'pose': 'Unspecified',\n",
            "                           'truncated': '0'},\n",
            "                'path': 'C:\\\\tensorflow1\\\\models\\\\research\\\\object_detection\\\\images\\\\test\\\\apple_79.jpg',\n",
            "                'segmented': '0',\n",
            "                'size': {'depth': '3', 'height': '0', 'width': '0'},\n",
            "                'source': {'database': 'Unknown'}}}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kjIzbR3oYeNh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "91303aa2-72a9-4203-99ba-a7fa9853e548"
      },
      "source": [
        "# новый вариант с ресайзом\n",
        "\n",
        "classes = {'apple' : 0, 'orange' : 1, 'banana' : 2, 'mixed' : 3}\n",
        "folder = {'train' : 'train', 'test' : 'validation'}\n",
        "OUTPUT_PATH = '/content/drive/MyDrive/torch/fruit_cropped1'\n",
        "\n",
        "def prepare_data(path, counter=0):\n",
        "    images = []\n",
        "    for filename in tqdm(os.listdir(path), total=len(os.listdir(path))): # last file .ipynd, not found\n",
        "        sp_filename = filename.split('.')\n",
        "\n",
        "        if sp_filename[1]=='xml':\n",
        "            fruit_data = []\n",
        "            image = cv2.imread(f\"{path}/{sp_filename[0]}.jpg\")\n",
        "            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) \n",
        "\n",
        "            with open(f'{path}/{filename}') as fd:   # open bbox\n",
        "                bb_file = json.loads(json.dumps(xmltodict.parse(fd.read())))\n",
        "                image_path = f\"{path}/{bb_file['annotation']['filename']}\"\n",
        "                data_type = folder[bb_file['annotation']['folder']] # train or validation\n",
        "                size_h = image.shape[0] # cv2 format of shape\n",
        "                size_w = image.shape[1]\n",
        "                if type(bb_file['annotation']['object']) == list:\n",
        "                    for bboxes in bb_file['annotation']['object']:\n",
        "                        xmin = float(bboxes['bndbox']['xmin'])\n",
        "                        xmax = float(bboxes['bndbox']['xmax'])\n",
        "                        ymin = float(bboxes['bndbox']['ymin'])\n",
        "                        ymax = float(bboxes['bndbox']['ymax']) # after this step we need to format bb to our format image\n",
        "                        # ptype = classes[bboxes['name']]\n",
        "                        w = xmax - xmin\n",
        "                        h = ymax - ymin\n",
        "                        x_center = xmin + w / 2 - 1 # -1 use when convert from vok to yolo\n",
        "                        y_center = ymin + h / 2 - 1\n",
        "                        w /= size_w  # norm\n",
        "                        h /= size_h\n",
        "                        x_center /= size_w \n",
        "                        y_center /= size_h\n",
        "\n",
        "                        fruit_data.append([x_center, y_center, w, h, bboxes['name']])\n",
        "                        \n",
        "                else:\n",
        "                    bboxes = bb_file['annotation']['object']\n",
        "                    xmin = float(bboxes['bndbox']['xmin'])\n",
        "                    xmax = float(bboxes['bndbox']['xmax'])\n",
        "                    ymin = float(bboxes['bndbox']['ymin'])\n",
        "                    ymax = float(bboxes['bndbox']['ymax']) # after this step we need to format bb to our format image\n",
        "                    # ptype = classes[bboxes['name']]\n",
        "                    w = xmax - xmin\n",
        "                    h = ymax - ymin\n",
        "                    x_center = xmin + w / 2 - 1  # -1 use when convert from vok to yolo\n",
        "                    y_center = ymin + h / 2 - 1 \n",
        "                    w /= size_w  # norm\n",
        "                    h /= size_h\n",
        "                    x_center /= size_w \n",
        "                    y_center /= size_h\n",
        "\n",
        "                    fruit_data.append([x_center, y_center, w, h, bboxes['name']])\n",
        "            transformed = transform(image=image, bboxes=fruit_data)\n",
        "            transformed_image = transformed['image']\n",
        "            transformed_bboxes = transformed['bboxes'] # to format, wich our network will understand\n",
        "            \n",
        "            for i, bbox in enumerate(transformed_bboxes):\n",
        "                transformed_bboxes[i] = [classes[bbox[-1]]] + list(bbox[:-1])\n",
        "\n",
        "            # now bbox in format [class, x_center, y_center, w, h]\n",
        "            fruit_data = np.array(transformed_bboxes)\n",
        "            np.savetxt(  \n",
        "                os.path.join(OUTPUT_PATH, f\"labels/{data_type}/{sp_filename[0]}.txt\"),\n",
        "                # os.path.join(OUTPUT_PATH, f\"labels/label_{counter}.txt\"),\n",
        "                fruit_data,\n",
        "                fmt=[\"%d\", \"%f\", \"%f\", \"%f\", \"%f\"] # no idea\n",
        "                )\n",
        "            cv2.imwrite(os.path.join(OUTPUT_PATH, f\"images/{data_type}/{sp_filename[0]}.jpg\"),\n",
        "                        transformed_image) # save image\n",
        "            counter += 1\n",
        "    return counter # return counter to start counting from end\n",
        "            \n",
        "\n",
        "\n",
        "last_num = prepare_data('/content/drive/MyDrive/torch/archive/test_zip/test')\n",
        "prepare_data('/content/drive/MyDrive/torch/archive/train_zip/train', last_num)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 121/121 [01:49<00:00,  1.10it/s]\n",
            "100%|██████████| 480/480 [06:38<00:00,  1.21it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "300"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6j0hpeBeYlv6"
      },
      "source": [
        "!python /content/yolov5/train.py --img 512 --batch 10 --epochs 30 --data /content/drive/MyDrive/torch/fruits.yaml --cfg /content/yolov5/models/yolov5x.yaml --weights yolov5x.pt --name yolov5x1cr"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bpmZIbcSYowa"
      },
      "source": [
        "!python /content/yolov5/detect.py --source /content/drive/MyDrive/torch/fruit_cropped1/images/validation/apple_77.jpg --weights /content/yolov5/runs/train/yolov5x1cr/weights/best.pt"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}